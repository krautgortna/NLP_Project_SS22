{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NLP_FakePolice.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NLP Project 2022"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/SakibShahriar95/ANTiVax/main/Labeled/VaxMisinfoData.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  !pip install twarc\n",
    "#  !twarc configure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Didn't work because of authentication"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "from twarc import Twarc\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "tweet_ids_url = 'https://raw.githubusercontent.com/SakibShahriar95/ANTiVax/main/Labeled/ids.txt'\n",
    "tweet_ids = urlretrieve(tweet_ids_url, './ids.txt') # it's a file like object and works just like a file\n",
    "\n",
    "df = pd.read_csv('./ids.txt')\n",
    "df.head()\n",
    "\n",
    "consumer_key=\"2lNba8Nvu9f0i5YgfmQhfh3mP\"\n",
    "consumer_secret=\"TGVv9QJhtkjSqm7QyMeLfpZm3IU3xjrrMabyae42VxbkaT5t0g\"\n",
    "access_token=\"1600730532-ZCeeHr9h033QRlC95BLZrpz85TRFN4pZ0w7vGhY\"\n",
    "access_token_secret=\"eoiufziPLBr0eYgR12gAoAEiTQjlnTkZ1BHBLXoDuQbtC\"\n",
    "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for tweet in t.hydrate(open('./ids.txt')):\n",
    "  row = { 'id': tweet['id'], 'text': tweet['text'] }\n",
    "  df.append(row, ignore_index = True)\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We hydrated offline and saved it to Github to retrieve it from there"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/krautgortna/NLP_Project_SS22/main/data/hydrated_tweets.csv'\n",
    "all_data = pd.read_csv(url)\n",
    "all_data.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Total = all_data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (all_data.isnull().sum() / all_data.isnull().count()).sort_values(ascending=False)\n",
    "missing_stats = pd.concat([Total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "all_data_drop_80_missing = all_data.drop((missing_stats[missing_stats['Percent'] > .8]).index, axis=1)\n",
    "missing_stats.head(25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged = df.merge(all_data[['id', 'text', 'hashtags']], on='id')\n",
    "df_merged\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(df_merged['hashtags'].str.split(\" \").explode('hashtags').unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_targets(review_text):\n",
    "    return re.sub('(@[A-Za-z0-9])\\w+', '', review_text)\n",
    "\n",
    "def remove_hashtags(review_text):\n",
    "    return review_text.replace(\"#\", \" \")\n",
    "\n",
    "def remove_numbers(review_text):\n",
    "    return re.sub('[0-9]', '', review_text)\n",
    "\n",
    "def remove_punctuation(review_text):\n",
    "    return re.sub(r'[/\\[\\]\\\\\\{\\}\\(\\)\\|\\;\\'\\:\\\"\\,\\!\\?\\.]', '', review_text)\n",
    "\n",
    "def remove_newlines(review_text):\n",
    "    return review_text.replace(\"\\n\", \" \")\n",
    "\n",
    "df_cleaned = df_merged.copy(deep=True)\n",
    "\n",
    "df_cleaned['text'] = df_cleaned['text'].apply(remove_targets)\n",
    "df_cleaned['text'] = df_cleaned['text'].apply(remove_hashtags)\n",
    "df_cleaned['text'] = df_cleaned['text'].apply(remove_numbers)\n",
    "\n",
    "df_cleaned['text'] = df_cleaned['text'].apply(remove_punctuation)\n",
    "df_cleaned['text'] = df_cleaned['text'].apply(remove_newlines)\n",
    "\n",
    "df_cleaned['text'] = df_cleaned['text'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "df_cleaned['text'] = df_cleaned['text'].str.lower()\n",
    "\n",
    "df_cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Debugging"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}